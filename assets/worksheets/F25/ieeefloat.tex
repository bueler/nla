\documentclass[11pt]{amsart}
%prepared in AMSLaTeX, under LaTeX2e
\addtolength{\oddsidemargin}{-.75in} 
\addtolength{\evensidemargin}{-.75in}
\addtolength{\topmargin}{-.4in}
\addtolength{\textwidth}{1.6in}
\addtolength{\textheight}{.9in}

\renewcommand{\baselinestretch}{1.05}

\usepackage{verbatim,fancyvrb}

\usepackage{tikz,palatino,amssymb}

\newtheorem*{thm}{Theorem}
\newtheorem*{defn}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{remark}{Remark}

\newcommand{\mtt}{\texttt}
\usepackage{alltt,xspace}
\newcommand{\mfile}[1]
{\medskip\begin{quote}\scriptsize \begin{alltt}\input{#1.m}\end{alltt} \normalsize\end{quote}\medskip}

%\usepackage[final]{graphicx}

\usepackage[pdftex, colorlinks=true, plainpages=false, linkcolor=blue, citecolor=red, urlcolor=blue]{hyperref}

% macros
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}

%\renewcommand{\det}{\operatorname{det}}
\newcommand{\onull}{\operatorname{null}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1.}\quad }
\newcommand{\exer}[2]{\prob{Exercise #2 in Lecture #1}}

\newcommand{\pts}[1]{(\emph{#1 pts}) }
\newcommand{\epart}[1]{\medskip\noindent\textbf{(#1)}\quad }
\newcommand{\ppart}[1]{\,\textbf{(#1)}\quad }

\newcommand{\Matlab}{\textsc{Matlab}\xspace}

\DefineVerbatimEnvironment{mVerb}{Verbatim}{numbersep=2mm,
frame=lines,framerule=0.1mm,framesep=2mm,xleftmargin=4mm,fontsize=\footnotesize}


\begin{document}
\scriptsize \noindent Math 614 Numerical Linear Algebra (Bueler) \hfill \fbox{\emph{Nothing to turn in!}}
\normalsize

\medskip\bigskip
\LARGE\centerline{IEEE 754: What it means for humanity and your computer}

\bigskip
\normalsize

\thispagestyle{empty}

Lecture 13 of the textbook\footnote{L.~Trefethen and D. Bau, \emph{Numerical Linear Algebra}, SIAM Press, 1997.} has an idealized view of floating point, which is wise.  However, in this separate document I lay out some basic details of how floating point numbers are \emph{actually} implemented on a computer, assuming they conform to the IEEE 754 standard, which they almost certainly do.

\bigskip
\begin{itemize}
\setlength\itemsep{1em}
\item A \emph{bit} is a binary digit, the irreducible atom of memory, always in either of two states $\{0,1\}$.  Computer memories are organized into \emph{bytes}, that is, groups of 8 bits.

\item \emph{Integers} are represented on computers using 1, 2, 4, or 8 bytes.  It is straightforward, and exact if the integer is not too big in magnitude, but we ignore the details.

\item The IEEE\footnote{IEEE $=$ Institute of Electrical and Electronics Engineers.} 754 standard regards how \emph{real} numbers are approximately represented in memory, that is, how \emph{floating point} numbers are represented.  ``Floating point'' is essentially just scientific notation, but using only finitely-many bits, so only a finite subset of real numbers can be represented.    For more information on the standard than described here see

 \centerline{\url{en.wikipedia.org/wiki/IEEE_754}}


\item The most important floating point representations use 32 or 64 bits, i.e.~4 pr 8 bytes.  These are called binary32 (also known as \texttt{single}) and binary64 (\texttt{double}) in the standard.  For binary32 the number
       $$x = (-1)^s \times \left(1.d_1 d_2 d_3 \dots d_{23}\right)_{2} \times 2^{\left(e_1\dots e_8\right)_2 - 127}$$
is represented by 32 bits this way:

\medskip
\hspace{-30mm}
    \begin{tikzpicture}[scale=0.6]
    \draw[xstep=1.0,ystep=1.0,gray,thin] (0.0,0.0) grid (32.0,1.0);
    \node at (0.5,0.5) {\scriptsize $s$};
    \foreach \x in {1,...,8} {
      \node at (0.5 + \x * 1.0,0.5) {\scriptsize $e_{\x}$};
    }
    \foreach \x in {1,...,23} {
      \node at (8.5 + \x * 1.0,0.5) {\scriptsize $d_{\x}$};
    }
    \end{tikzpicture}

\noindent In binary64 (\texttt{double}) the number
       $$x = (-1)^s \times \left(1.d_1 d_2 d_3 \dots d_{52}\right)_{2} \times 2^{\left(e_1\dots e_{11}\right)_2 - 1023}$$
is represented by 64 bits this way:

\medskip
    \begin{tikzpicture}[scale=0.6]
    \draw[xstep=1.0,ystep=1.0,gray,thin] (0.0,0.0) grid (25.0,1.0);
    \node at (0.5,0.5) {\scriptsize $s$};
    \foreach \x in {1,...,11} {
      \node at (0.5 + \x * 1.0,0.5) {\scriptsize $e_{\x}$};
    }
    \foreach \x in {1,...,10} {
      \node at (11.5 + \x * 1.0,0.5) {\scriptsize $d_{\x}$};
    }
    \node at (22.5,0.5) {\scriptsize $\dots$};
    \node at (23.5,0.5) {\scriptsize $d_{51}$};
    \node at (24.5,0.5) {\scriptsize $d_{52}$};
    \end{tikzpicture}

\item Note that the ``$1.$'' in the above representations, which appears before the $d_i$ bits, is always present and therefore it does \emph{not} use a bit of memory!  It is called the ``implicit leading bit''.

\item Unlike the system $\mathbb{F}$ in the textbook (Lecture 13), in any actual floating-point representation, including IEEE 754, there are only finitely-many allowed values of the exponent $e$.  Thus there are only finitely-many representable floating point numbers.

\item The IEEE 754 standard uses abstract language to describe the way the digits are arranged.  Every representable \emph{nonzero} number is of the form
\begin{equation}
x = (-1)^s \times \frac{m}{\beta^{t-1}} \times \beta^e  \label{ieeeform}
\end{equation}
for fixed positive integers $\beta$ (the \emph{base}) and $t$ (the \emph{precision}).  The other symbols are $s\in\{0,1\}$ (the \emph{sign}), an integer $m$ (the \emph{mantissa}), and an integer $e$ (the \emph{exponent}).  These satisfy
\begin{equation}
\beta^{t-1} \le m \le \beta^t - 1, \qquad e_{min} \le e \le e_{max}.  \label{ieeeconstraint}
\end{equation}

\item In the current version of the standard, IEEE 754-2019, there are a number of formats.  However, we ignore the \emph{decimal} standards with $\beta=10$, which are rarely used.  We consider only \emph{binary} formats with $\beta=2$.  The formats that matter most use 16, 32, 64, or 128 bits.  We have already shown how the first two are implemented in memory.  In terms of \eqref{ieeeform} and \eqref{ieeeconstraint} they follow this table:

\bigskip
\small
\begin{tabular}{lllllll}
name     & common name & precision $t$ & exponent bits & exponent bias & $e_{min}$ & $e_{max}$ \\ \hline
binary16 &      \texttt{half} & 11 &  5 &      $2^4-1=15$ &   -14 &   +15 \\
binary32 &    \texttt{single} & 24 &  8 &     $2^7-1=127$ &  -126 &  +127 \\
binary64 &    \texttt{double} & 53 & 11 & $2^{10}-1=1023$ & -1022 & +1023 \\
binary128 &\texttt{quadruple} &113 & 15 &$2^{14}-1=16383$ &-16382 &+16383
\end{tabular}
\normalsize
\medskip

\item If you convert the precision and exponent limits to decimal you get these values:

\bigskip
\small
\begin{tabular}{llll}
name & decimal precision & decimal $e_{max}$ & decimal $e_{min}$ \\ \hline
binary16 & 3.31 & 4.51 & -4.21 \\
binary32 & 7.22 & 38.23 & -37.93 \\
binary64 & 15.95 & 307.95 & -307.65 \\
binary128 & 34.02 & 4931.77 & -4931.47
\end{tabular}
\normalsize
\medskip

\item For \emph{normal} numbers, in \texttt{single} the standard requires $\left(e_1\dots e_8\right)_2 \in \{1,2,\dots,254\}$ and in \texttt{double} the standard requires $\left(e_1\dots e_{11}\right)_2 \in \{1,2,\dots,2046\}$.

\item If all bits $e_i$ are zero or all bits $e_i$ are one then the number has special/exceptional meaning.  Note that the number zero is \emph{not} in form \eqref{ieeeform}, and it is such an exception.  Zero is represented by setting all bits other than $s$ to zero.  Because the sign bit is not determined, this means ``$+0$'' and ``$-0$'' exist as separate representations.  (Strange but true!)

\item For a ``subnormal'' number, all the bits $e_i$ are zero but some bits $d_i$ are nonzero.

\item When all bits $e_i$ are one there are representations of $+\infty$ and $-\infty$ and of things that are ``not a number'' (\texttt{NaN}).  We omit all details here.  

\item It is safe to assume that any modern desktop or laptop computer implements IEEE-compliant binary64 floating point operations \emph{in hardware}.  Other types are commonly implemented in software, especially binary128, which is thus much slower on current computers.  The smaller binary16 and binary32 formats are typically used for \emph{storing} numbers, which increases storage capacity, or for graphics, or for the weights in a neural network.

\item One major goal of the IEEE 754 standard is that axiom (13.5) in the textbook applies.  The standard also addresses the rounding errors which occur in arithmetic operations (addition, multiplication, etc.), with the goal that axiom (13.7) in the textbook applies.  In fact, the design goal is that the two axioms hold with as small a value for $\eps_{\text{machine}}$ as practically possible.  Taking this viewpoint, the other details are not so important.

\end{itemize}

\end{document}


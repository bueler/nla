\documentclass{amsart}

\addtolength{\oddsidemargin}{-0.9in} 
\addtolength{\evensidemargin}{-0.9in}
\addtolength{\topmargin}{-0.6in}
\addtolength{\textwidth}{1.4in}
\addtolength{\textheight}{1.4in}

\usepackage{verbatim}
\usepackage{color}
\usepackage{palatino}

% inclusion/figure macros
\usepackage{graphicx}

\usepackage{amsmath}
% macros
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\DivH}{\nabla_H\cdot}
\newcommand{\ddx}[1]{\frac{\partial #1}{\partial x}}
\newcommand{\ddy}[1]{\frac{\partial #1}{\partial y}}
\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\gradH}{\nabla_H}
\newcommand{\ip}[2]{\left<#1,#2\right>}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}
%\newcommand{\qed}{\emph{ q.e.d.}}
\newcommand{\real}{\mathbf{R}}
\newcommand{\vf}{\varphi}
%\renewcommand{\note}[1]{\tiny{\textbf{NOTE: #1}.}}

\newcommand{\alert}[1]{{\color{red} #1}}

\begin{document}

\Huge
\centerline{Matrix Norm Essentials}

\thispagestyle{empty}

\bigskip\bigskip
\LARGE
\begin{itemize}
\setlength\itemsep{1em}
\item All matrix norms have \alert{vector norm properties}:
  \begin{itemize}
  \item[$\circ$] $\|A\| \ge 0$ and $\|A\|=0 \,\implies \,A=0$
  \item[$\circ$] $\|A+B\| \le \|A\| + \|B\|$
  \item[$\circ$] $\|\alpha A\| = |\alpha|\,\|A\|$
  \end{itemize}
\item \alert{Induced matrix norms} arise from vector norms.  For $A\in\CC^{m\times n}$,
	$$\|A\| = \sup_{\begin{smallmatrix} x\in\CC^n \\ x\ne 0 \end{smallmatrix}} \frac{\|Ax\|_{(m)}}{\|x\|_{(n)}} = \max_{\|x\|_{(n)}=1} \|Ax\|_{(m)}$$
\item This follows directly from the definition of an induced norm:
  \begin{itemize}
  \item[$\circ$] $\|Ax\|_{(m)} \le \|A\| \|x\|_{(n)}$
  \end{itemize}
\item There are really only \alert{four} matrix norms to know:
  $$\|\cdot\|_1, \quad \|\cdot\|_2, \quad \|\cdot\|_\infty, \quad \|\cdot\|_{\mathrm{Fro}}$$
  \begin{itemize}
  \item[$\circ$] 3 are \alert{induced} from vector norms: \quad $1,2,\infty$
  \item[$\circ$] 3 have \alert{easy-to-compute formulas}: \quad $1,\infty,\mathrm{Fro}$
  \item[$\circ$] {\Large \texttt{norm(A,1|Inf|"fro")}} are fast in Matlab, {\Large \texttt{norm(A,2)}} is slow
  \end{itemize}
\item Induced norms (\emph{and} Frobenius) have a \alert{multiplicative property}:
  \begin{itemize}
  \item[$\circ$] $\|A B\|\le \|A\|\|B\|$
  \end{itemize}
\item Induced norms (\emph{and} Frobenius) satisfy \alert{$\rho(A)\le \|A\|$}.
  \begin{itemize}
  \item[$\circ$] Recall $\rho(A) = \max |\lambda|$ where $\lambda$ is an eigenvalue.
  \item[$\circ$] However, $\rho(A)\ll\|A\|$ is common.  The norm can be a very conservative, that is, too large, estimate of $\rho(A)$.
  \end{itemize}
\item The $\|\cdot\|_2$ norm is best for \alert{Euclidean ideas} and \alert{hermitian/normal matrices}.  Reasons: 
  \begin{itemize}
  \item[$\circ$]  $\|QA\|_2 = \|A\|_2$ if $Q$ is unitary ($Q^* Q = I$).
  \item[$\circ$]  Largest singular value: $\sigma_1(A) = \|A\|_2$.
  \item[$\circ$]  If $A^*=A$ then $\rho(A)=\|A\|_2$.
  \end{itemize}
\item \alert{Iteration} fact:
    $$v, Av, A^2v, \dots \text{ converges for all $v$}$$
    $$\text{if and only if } \rho(A)<1.$$
  \begin{itemize}
  \item[$\circ$] Thus \alert{if} $\|A\|<1$ \alert{then} convergence.
  \item[$\circ$] But not conversely!
  \end{itemize}
\end{itemize}

\end{document}

